{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk+jA79f3wk2NAVGAUYKhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ysydso/newtest/blob/main/Innoprenuer_final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install neccessary libraries**"
      ],
      "metadata": {
        "id": "9YkHamBWyaFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JKYJeyuZ2W7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install ultralytics opencv-python-headless pandas matplotlib torch torchvision pyyaml"
      ],
      "metadata": {
        "id": "WoSdNWbqyWnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the required libraries**"
      ],
      "metadata": {
        "id": "MgHIfvJlysW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from sklearn.model_selection import KFold\n",
        "import yaml\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "1cxaZbayzIQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up logging system to track model progress**"
      ],
      "metadata": {
        "id": "trmmZwZ0zLN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('training.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2_ITWttFzOca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuration class for organization**"
      ],
      "metadata": {
        "id": "YRsJBc1BzSdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class YOLOConfig:\n",
        "    k_folds: int\n",
        "    batch_size: int\n",
        "    epochs: int\n",
        "    patience: int\n",
        "    image_size: int\n",
        "    model_type: str\n",
        "    checkpoint_dir: str\n",
        "    num_workers: int\n",
        "    pin_memory: bool\n",
        "    device: str\n",
        "\n",
        "    @classmethod\n",
        "    def create_default(cls):\n",
        "        return cls(\n",
        "            k_folds=5,\n",
        "            batch_size=16,\n",
        "            epochs=50,\n",
        "            patience=5,\n",
        "            image_size=640,\n",
        "            model_type='yolov8n-seg.yaml',\n",
        "            checkpoint_dir='/content/checkpoints',\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        )"
      ],
      "metadata": {
        "id": "54XyLRRYzWPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics tracking class**"
      ],
      "metadata": {
        "id": "0OAj6LQJzazv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.metrics_history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'map50': [],\n",
        "            'map95': []\n",
        "        }\n",
        "        self.best_metrics = {\n",
        "            'best_map50': 0,\n",
        "            'best_map95': 0,\n",
        "            'lowest_val_loss': float('inf')\n",
        "        }\n",
        "\n",
        "    def update(self, metrics: Dict[str, float]):\n",
        "        \"\"\"Update metrics history and best metrics\"\"\"\n",
        "        for key, value in metrics.items():\n",
        "            if key in self.metrics_history:\n",
        "                self.metrics_history[key].append(value)\n",
        "\n",
        "        if metrics.get('map50', 0) > self.best_metrics['best_map50']:\n",
        "            self.best_metrics['best_map50'] = metrics['map50']\n",
        "        if metrics.get('map95', 0) > self.best_metrics['best_map95']:\n",
        "            self.best_metrics['best_map95'] = metrics['map95']\n",
        "        if metrics.get('val_loss', float('inf')) < self.best_metrics['lowest_val_loss']:\n",
        "            self.best_metrics['lowest_val_loss'] = metrics['val_loss']\n"
      ],
      "metadata": {
        "id": "lOybWHrIzd0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early stopping class**"
      ],
      "metadata": {
        "id": "-UeFZ0nnzl8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            return False\n",
        "\n",
        "        if val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "        return self.should_stop"
      ],
      "metadata": {
        "id": "ZFEp1BjXznwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset validation function**"
      ],
      "metadata": {
        "id": "XfKHPEPOzrHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(data_yaml_path: str) -> bool:\n",
        "    \"\"\"Validate the YOLO dataset structure and yaml file.\"\"\"\n",
        "    try:\n",
        "        with open(data_yaml_path, 'r') as f:\n",
        "            data_config = yaml.safe_load(f)\n",
        "\n",
        "        required_keys = ['path', 'train', 'val', 'names']\n",
        "        if not all(key in data_config for key in required_keys):\n",
        "            logging.error(f\"Missing required keys in {data_yaml_path}\")\n",
        "            return False\n",
        "\n",
        "        # Convert relative paths to absolute\n",
        "        base_path = Path(data_config['path']).resolve()\n",
        "        for split in ['train', 'val']:\n",
        "            split_path = base_path / data_config[split]\n",
        "            if not split_path.exists():\n",
        "                logging.error(f\"Missing {split} dataset path: {split_path}\")\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Dataset validation failed: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "UZ9-_AZPzs8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint save function**"
      ],
      "metadata": {
        "id": "S4-PKNs5z1GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, fold, epoch, train_loss, val_loss, metrics, config, is_best=False):\n",
        "    \"\"\"Save model checkpoint with additional metrics\"\"\"\n",
        "    checkpoint = {\n",
        "        'fold': fold,\n",
        "        'epoch': epoch,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'metrics': metrics,\n",
        "        'model_state': model.state_dict() if hasattr(model, 'state_dict') else None,\n",
        "    }\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Save regular checkpoint\n",
        "    checkpoint_path = f\"{config.checkpoint_dir}/fold_{fold}_epoch_{epoch}.pt\"\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    logging.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Save best model separately\n",
        "    if is_best:\n",
        "        best_path = f\"{config.checkpoint_dir}/fold_{fold}_best_model.pt\"\n",
        "        torch.save(checkpoint, best_path)\n",
        "        logging.info(f\"Saved best model: {best_path}\")\n",
        "\n",
        "# Function to load checkpoint\n",
        "def load_checkpoint(model, fold, config):\n",
        "    \"\"\"Load the latest checkpoint for a specific fold\"\"\"\n",
        "    try:\n",
        "        checkpoints = [f for f in os.listdir(config.checkpoint_dir)\n",
        "                      if f.startswith(f'fold_{fold}_epoch_')]\n",
        "\n",
        "        if not checkpoints:\n",
        "            return 0, float('inf'), {}\n",
        "\n",
        "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_epoch_')[1].split('.')[0]))\n",
        "        checkpoint_path = os.path.join(config.checkpoint_dir, latest_checkpoint)\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        if hasattr(model, 'load_state_dict'):\n",
        "            model.load_state_dict(checkpoint['model_state'])\n",
        "\n",
        "        logging.info(f\"Loaded checkpoint: {checkpoint_path}\")\n",
        "        return checkpoint['epoch'] + 1, checkpoint['val_loss'], checkpoint.get('metrics', {})\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load checkpoint: {str(e)}\")\n",
        "        return 0, float('inf'), {}"
      ],
      "metadata": {
        "id": "L2h4Pylsz3WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction visualization function**"
      ],
      "metadata": {
        "id": "YtjQGIcdz_ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(image: np.ndarray, predictions: list, output_path: str):\n",
        "    \"\"\"Visualize detection results on the image\"\"\"\n",
        "    try:\n",
        "        img_copy = image.copy()\n",
        "        for pred in predictions:\n",
        "            box = pred['box'].astype(int)\n",
        "            conf = pred['confidence']\n",
        "            cv2.rectangle(img_copy, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "            cv2.putText(img_copy, f\"{conf:.2f}\", (box[0], box[1]-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        cv2.imwrite(output_path, img_copy)\n",
        "        logging.info(f\"Saved visualization to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Visualization failed: {str(e)}\")\n",
        "\n",
        "def evaluate_model(model: YOLO, test_images_path: str) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate model performance on a test set\"\"\"\n",
        "    try:\n",
        "        results = model.val(data=test_images_path)\n",
        "        metrics = {\n",
        "            'precision': results.results_dict.get('metrics/precision(B)', 0),\n",
        "            'recall': results.results_dict.get('metrics/recall(B)', 0),\n",
        "            'mAP50': results.results_dict.get('metrics/mAP50(B)', 0),\n",
        "            'mAP95': results.results_dict.get('metrics/mAP50-95(B)', 0)\n",
        "        }\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Evaluation failed: {str(e)}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "B4dti-4D0COY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main machine learning setup**"
      ],
      "metadata": {
        "id": "1qVWaVPL0Gu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Initialize configuration\n",
        "        config = YOLOConfig.create_default()\n",
        "\n",
        "        # Extract dataset\n",
        "        with zipfile.ZipFile(\"/content/banana01.v3i.yolov8.zip\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/yolo_dataset\")\n",
        "        data_yaml_path = \"/content/yolo_dataset/data.yaml\"\n",
        "        logging.info(\"Dataset extracted successfully\")\n",
        "\n",
        "        # Validate dataset\n",
        "        if not validate_dataset(data_yaml_path):\n",
        "            raise ValueError(\"Dataset validation failed\")\n",
        "\n",
        "        # Initialize YOLO model\n",
        "        yolo_model = YOLO(config.model_type)\n",
        "        logging.info(\"YOLO model initialized successfully\")\n",
        "\n",
        "        # Initialize metrics tracker\n",
        "        metrics_tracker = MetricsTracker()\n",
        "\n",
        "        # Set up K-Fold cross-validation\n",
        "        kfold = KFold(n_splits=config.k_folds, shuffle=True)\n",
        "\n",
        "        # Training loop with K-Fold Cross-Validation\n",
        "        best_models = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kfold.split(range(config.epochs))):\n",
        "            logging.info(f'Starting training fold {fold + 1}/{config.k_folds}')\n",
        "\n",
        "            # Load checkpoint if exists\n",
        "            start_epoch, best_val_loss, previous_metrics = load_checkpoint(yolo_model, fold, config)\n",
        "\n",
        "            # Initialize early stopping\n",
        "            early_stopping = EarlyStopping(patience=config.patience)\n",
        "\n",
        "            # Training configuration for this fold\n",
        "            training_config = {\n",
        "                'data': data_yaml_path,\n",
        "                'epochs': config.epochs,\n",
        "                'imgsz': config.image_size,\n",
        "                'batch': config.batch_size,\n",
        "                'name': f'banana_detection_model_fold_{fold}',\n",
        "                'patience': config.patience,\n",
        "                'resume': start_epoch > 0,\n",
        "                'device': config.device,\n",
        "                'workers': config.num_workers,\n",
        "                'pin_memory': config.pin_memory\n",
        "            }"
      ],
      "metadata": {
        "id": "HjiExv520JPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main machine learning execution**"
      ],
      "metadata": {
        "id": "MqytKnwW0bcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            results = yolo_model.train(**training_config)"
      ],
      "metadata": {
        "id": "oBEWIj_30l2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics update function**"
      ],
      "metadata": {
        "id": "f1zgLtGe0oFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            metrics = {\n",
        "                'map50': results.results_dict.get('metrics/mAP50(B)', 0),\n",
        "                'map95': results.results_dict.get('metrics/mAP50-95(B)', 0),\n",
        "                'val_loss': results.results_dict.get('val/box_loss', 0)\n",
        "            }\n",
        "            metrics_tracker.update(metrics)"
      ],
      "metadata": {
        "id": "RTawH6oJ0ygm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint save**"
      ],
      "metadata": {
        "id": "cmX6ryVu03EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            save_checkpoint(\n",
        "                yolo_model,\n",
        "                fold,\n",
        "                config.epochs,\n",
        "                results.results_dict.get('train/box_loss', 0),\n",
        "                metrics['val_loss'],\n",
        "                metrics,\n",
        "                config,\n",
        "                is_best=metrics['map50'] > metrics_tracker.best_metrics['best_map50']\n",
        "            )\n",
        "\n",
        "            # Save model for this fold\n",
        "            model_path = f'banana_detection_model_fold_{fold}.pt'\n",
        "            yolo_model.save(model_path)\n",
        "\n",
        "            best_models.append({\n",
        "                'fold': fold,\n",
        "                'model_path': model_path,\n",
        "                'metrics': metrics\n",
        "            })\n",
        "\n",
        "            logging.info(f'Completed training fold {fold + 1}')\n",
        "            logging.info(f'Best metrics for fold {fold + 1}: {metrics_tracker.best_metrics}')"
      ],
      "metadata": {
        "id": "FFemdYMh05DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early stopping with final evaluation**"
      ],
      "metadata": {
        "id": "5Atn6Aqd1Bxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        # Check early stopping\n",
        "            if early_stopping(metrics['val_loss']):\n",
        "                logging.info(f\"Early stopping triggered in fold {fold + 1}\")\n",
        "                break\n",
        "\n",
        "        # Final evaluation\n",
        "        logging.info(\"Training completed. Final metrics:\")\n",
        "        for metric, value in metrics_tracker.best_metrics.items():\n",
        "            logging.info(f\"{metric}: {value}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Training failed: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "bFVB541K1GT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sessions statistics class**"
      ],
      "metadata": {
        "id": "jk0O1EoH1KkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionStatistics:\n",
        "    def __init__(self):\n",
        "        self.total_bananas = 0\n",
        "        self.counts = {\"Unripe\": 0, \"Ripe\": 0, \"Overripe\": 0, \"Rotten\": 0}\n",
        "        self.timestamps = []\n",
        "        self.confidence_scores = []\n",
        "\n",
        "    def update(self, ripeness, confidence=None):\n",
        "        if ripeness not in self.counts:\n",
        "            logging.error(f\"Invalid ripeness value: {ripeness}\")\n",
        "            return\n",
        "\n",
        "        self.total_bananas += 1\n",
        "        self.counts[ripeness] += 1\n",
        "        self.timestamps.append(datetime.now())\n",
        "        if confidence is not None:\n",
        "            self.confidence_scores.append(confidence)\n",
        "\n",
        "    def calculate_quality_score(self):\n",
        "        if self.total_bananas == 0:\n",
        "            return 0\n",
        "        rotten_count = self.counts[\"Rotten\"]\n",
        "        overripe_count = self.counts[\"Overripe\"]\n",
        "        quality_score = 100 - ((rotten_count + overripe_count) / self.total_bananas) * 100\n",
        "        return quality_score\n",
        "\n",
        "    def get_summary(self):\n",
        "        quality_score = self.calculate_quality_score()\n",
        "        rotten_percentage = (self.counts[\"Rotten\"] / max(self.total_bananas, 1)) * 100\n",
        "\n",
        "        summary = {\n",
        "            \"Total Bananas\": self.total_bananas,\n",
        "            \"Unripe\": self.counts[\"Unripe\"],\n",
        "            \"Ripe\": self.counts[\"Ripe\"],\n",
        "            \"Overripe\": self.counts[\"Overripe\"],\n",
        "            \"Rotten\": self.counts[\"Rotten\"],\n",
        "            \"Rotten Percentage\": rotten_percentage,\n",
        "            \"Quality Score\": quality_score,\n",
        "            \"Session Timestamp\": self.timestamps[0] if self.timestamps else \"N/A\",\n",
        "            \"Average Confidence\": np.mean(self.confidence_scores) if self.confidence_scores else 0\n",
        "        }\n",
        "\n",
        "        return summary"
      ],
      "metadata": {
        "id": "ZnigK-HGymuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Database class**"
      ],
      "metadata": {
        "id": "nMIvwPB81RE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDatabase:\n",
        "    def __init__(self):\n",
        "        self.sessions = []\n",
        "\n",
        "    def log_session(self, session_stats):\n",
        "        try:\n",
        "            self.sessions.append(session_stats.get_summary())\n",
        "            logging.info(\"Session logged successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to log session: {str(e)}\")\n",
        "\n",
        "    def retrieve_sessions(self):\n",
        "        return pd.DataFrame(self.sessions)\n",
        "\n",
        "    def get_statistics(self):\n",
        "        df = self.retrieve_sessions()\n",
        "        return {\n",
        "            'total_sessions': len(df),\n",
        "            'total_bananas': df['Total Bananas'].sum(),\n",
        "            'average_quality_score': df['Quality Score'].mean(),\n",
        "            'average_confidence': df['Average Confidence'].mean()\n",
        "        }"
      ],
      "metadata": {
        "id": "wUGRphPt1Vv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Hgsu1OzynLJ"
      }
    }
  ]
}